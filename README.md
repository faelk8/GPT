
<a href="#"></a>

<div style="text-align: center;">
  <h1>GPT</h1>
</div>


# ğŸ§  Sobre este RepositÃ³rio
Este repositÃ³rio Ã© um guia estruturado e prÃ¡tico sobre os fundamentos e a implementaÃ§Ã£o do modelo GPT (Generative Pre-trained Transformer). Nele, exploramos desde os conceitos iniciais como tokenizaÃ§Ã£o e tokens especiais, atÃ© os componentes centrais da arquitetura Transformer, como self-attention, multi-head attention e mÃ¡scara causal.

AlÃ©m disso, demonstramos a execuÃ§Ã£o do GPT-2 com exemplos de tokenizaÃ§Ã£o, geraÃ§Ã£o de texto, e estratÃ©gias de avaliaÃ§Ã£o e treinamento de modelos generativos, incluindo o uso de pesos prÃ©-treinados da OpenAI.

## ğŸ“š Menu
<ul>
  <li><a href="#01">ğŸ”¤ 01 - TokenizaÃ§Ã£o</a>
    <ul>
      <li><a href="#01.01">ğŸ”– Tokens especiais</a></li>
      <li><a href="#01.02">ğŸ”– ğŸ§¬ TÃ©cnica de TokenizaÃ§Ã£o â€” Byte Pair Encoding (BPE)</a></li>
    </ul>
  </li>

  <li><a href="#02">ğŸ§  02 - Attention</a>
    <ul>
      <li><a href="#02.01">ğŸ§© Como funciona a atenÃ§Ã£o?</a></li>
      <li><a href="#02.02">ğŸ¯ Multi-head Attention</a></li>
      <li><a href="#02.03">âš™ï¸ Exemplo prÃ¡tico</a></li>
    </ul>
  </li>

  <li><a href="#03">ğŸ§  3 - Attention o desafio de modelar sequÃªncias longas</a>
    <ul>
      <li><a href="#03.01">ğŸ” Self-Attention: AtenÃ§Ã£o a diferentes partes da entrada</a></li>
      <li><a href="#03.02">âœ… ImplementaÃ§Ã£o simples sem pesos treinÃ¡veis</a></li>
      <li><a href="#03.03">ğŸ“Š CÃ¡lculo de pesos de atenÃ§Ã£o</a></li>
      <li><a href="#03.04">âš™ï¸ ImplementaÃ§Ã£o com pesos treinÃ¡veis</a></li>
      <li><a href="#03.05">ğŸ•¶ï¸ Causal Attention: ocultando palavras futuras</a></li>
      <li><a href="#03.06">ğŸ”’ AplicaÃ§Ã£o da mÃ¡scara causal</a></li>
      <li><a href="#03.07">ğŸŒ§ï¸ Dropout na atenÃ§Ã£o</a></li>
      <li><a href="#03.08">ğŸ§© Multi-Head Attention: atenÃ§Ã£o paralela</a></li>
      <li><a href="#03.09">ğŸ§± Stacking de camadas de atenÃ§Ã£o</a></li>
      <li><a href="#03.10">ğŸ“¦ ImplementaÃ§Ã£o Compacta</a></li>
    </ul>
  </li>

  <li><a href="#04">ğŸ”„ 4 - ExecuÃ§Ã£o do Modelo GPT-2 com TokenizaÃ§Ã£o e GeraÃ§Ã£o de Texto</a>
    <ul>
      <li><a href="#04.01">ğŸ§  TokenizaÃ§Ã£o</a></li>
      <li><a href="#04.02">âš™ï¸ ConfiguraÃ§Ã£o do Modelo</a></li>
      <li><a href="#04.03">ğŸš€ ExecuÃ§Ã£o e SaÃ­da</a></li>
      <li><a href="#04.04">âœï¸ GeraÃ§Ã£o de Texto</a></li>
    </ul>
  </li>

  <li><a href="#05">âœ¨ AvaliaÃ§Ã£o e Treinamento de Modelos de Texto Generativo</a>
    <ul>
      <li><a href="#05.01">â­ AvaliaÃ§Ã£o de Modelos de Texto Generativo</a></li>
      <li><a href="#05.02">ğŸ’ª Treinamento de um LLM (Large Language Model)</a></li>
      <li><a href="#05.03">â™Ÿï¸ EstratÃ©gias de DecodificaÃ§Ã£o para Controlar Aleatoriedade</a></li>
      <li><a href="#05.04">ğŸ—ï¸ Carregamento de Pesos PrÃ©-Treinados da OpenAI</a></li>
    </ul>
  </li>

  <li><a href="#06">âœ‰ï¸ 06 - Spam</a></li>
  <li><a href="#07">ğŸ’ª 07 - InstruÃ§Ãµes para Treinamento</a></li>
  <li><a href="#08">ReferÃªncia</a></li>
</ul>


##

<h1 id="01">ğŸ”¤ 1 - TokenizaÃ§Ã£o</h1>
Antes que um modelo de linguagem possa processar texto, Ã© necessÃ¡rio transformar as palavras em tokens, ou seja, converter o texto em unidades menores que o modelo consegue entender e manipular. Isso Ã© feito atravÃ©s de um tokenizer. ApÃ³s o processamento, os tokens podem ser convertidos de volta para texto, permitindo reconstruir a saÃ­da gerada pelo modelo em linguagem natural.

<h3 id="01.01">ğŸ”– Tokens especiais </h3> 
Durante a tokenizaÃ§Ã£o, alguns tokens especiais sÃ£o usados para indicar posiÃ§Ãµes especÃ­ficas em uma sequÃªncia de texto:

* [BOS] (Beginning of Sequence) â€” Marca o inÃ­cio da sequÃªncia. Esse token informa ao modelo onde o conteÃºdo comeÃ§a.
* [EOS] (End of Sequence) â€” Indica o fim da sequÃªncia. Ele Ã© Ãºtil especialmente quando se juntam vÃ¡rios textos, como artigos ou sentenÃ§as diferentes. Semelhante ao token <|endoftext|> utilizado no GPT-2.
* [PAD] (Padding) â€” Usado para preencher textos mais curtos durante o treinamento em lotes. Quando sequÃªncias possuem tamanhos diferentes, os textos mais curtos sÃ£o estendidos com esse token para igualar o comprimento do lote.

<h3 id="01.02">ğŸ§¬ TÃ©cnica de TokenizaÃ§Ã£o â€” Byte Pair Encoding (BPE)</h3> 
O mÃ©todo mais comum utilizado na tokenizaÃ§Ã£o de modelos como o GPT-2 Ã© o Byte Pair Encoding (BPE). O BPE divide palavras em subpalavras ou caracteres com base em frequÃªncia de ocorrÃªncia. 
Palavras comuns sÃ£o representadas por menos tokens, enquanto palavras raras sÃ£o quebradas em partes menores.
Exemplo:
* A palavra desconectado pode ser tokenizada como: ["des", "con", "ect", "ado"].

Esse mÃ©todo Ã© eficiente porque reduz o vocabulÃ¡rio necessÃ¡rio e permite ao modelo lidar com palavras desconhecidas ou inventadas de maneira mais robusta.

<div style="text-align: right;">
  <a href="#">ğŸ” Voltar ao topo</a>
</div>

<h1 id="02">ğŸ§  2 - Attention</h1>
Camada de AtenÃ§Ã£o (Attention Layer)
O modelo GPT-2 Ã© baseado em uma arquitetura chamada Transformer, cujo principal componente Ã© a camada de atenÃ§Ã£o. Essa camada permite que o modelo "preste atenÃ§Ã£o" em diferentes partes da entrada enquanto estÃ¡ processando uma palavra ou token, capturando relaÃ§Ãµes de contexto de curto e longo alcance.

<h3 id="02.01">ğŸ§© Como funciona a atenÃ§Ã£o?</h3> 
Durante o treinamento, cada token da entrada Ã© transformado em vetores chamados de query (Q), key (K) e value (V). A atenÃ§Ã£o Ã© calculada comparando as queries de um token com as keys dos outros tokens â€” permitindo que o modelo decida quais palavras sÃ£o mais relevantes para prever a prÃ³xima.

<h3 id="02.02">ğŸ¯ Multi-head Attention</h3> 
Em vez de ter uma Ãºnica "cabeÃ§a de atenÃ§Ã£o", o Transformer pode usar vÃ¡rias cabeÃ§as (como 2, 4, 8 ou mais). Cada cabeÃ§a aprende padrÃµes diferentes de dependÃªncia entre palavras. Por exemplo:

* 1 cabeÃ§a: o modelo foca em um Ãºnico padrÃ£o de contexto.
* 2 cabeÃ§as: o modelo pode aprender dois padrÃµes distintos ao mesmo tempo, como relaÃ§Ãµes sintÃ¡ticas e semÃ¢nticas.

No GPT-2, essa atenÃ§Ã£o multi-cabeÃ§a Ã© uma das razÃµes pela qual ele entende tÃ£o bem o contexto de uma frase, mesmo quando as palavras estÃ£o distantes.

<h3 id="02.03">âš™ï¸ Exemplo prÃ¡tico</h3> </h3> 
Se vocÃª estiver usando n_heads=2, o modelo divide o vetor de entrada em duas partes, aplica a atenÃ§Ã£o separadamente em cada uma, e depois concatena os resultados. Isso melhora a capacidade do modelo de capturar diferentes tipos de dependÃªncias linguÃ­sticas simultaneamente.

<div style="text-align: right;">
  <a href="#">ğŸ” Voltar ao topo</a>
</div>


<h1 id="03">ğŸ§  3 - Attention o desafio de modelar sequÃªncias longas</h1>
Modelos de linguagem precisam lidar com sequÃªncias de texto de comprimento variÃ¡vel, mas muitos mÃ©todos tradicionais (como RNNs) sofrem com limitaÃ§Ãµes no alcance de dependÃªncias longas â€” ou seja, perdem informaÃ§Ãµes importantes quando os tokens estÃ£o muito distantes uns dos outros.<br>
O mecanismo de self-attention (captura dependÃªncias com mecanismos de atenÃ§Ã£o) resolve esse problema ao permitir que cada palavra "atenda" a todas as outras palavras da sequÃªncia, capturando relacionamentos globais entre os tokens de forma eficiente.
No self-attention, cada token gera uma consulta (query), chave (key) e valor (value), permitindo o cÃ¡lculo de pesos de atenÃ§Ã£o para todas as outras posiÃ§Ãµes da sequÃªncia. Com isso, o modelo consegue:
* Focar nos tokens relevantes
* Ignorar informaÃ§Ãµes irrelevantes
* Preservar contexto global em uma Ãºnica camada

<h3 id="03.02">âœ… ImplementaÃ§Ã£o simples sem pesos treinÃ¡veis</h3> 
VocÃª pode implementar uma versÃ£o bÃ¡sica de self-attention que apenas calcula as similaridades entre os tokens, sem ajustar pesos.

<h3 id="03.03">ğŸ“Š CÃ¡lculo de pesos de atenÃ§Ã£o</h3> 
O cÃ¡lculo dos pesos segue os seguintes passos:
* MultiplicaÃ§Ã£o de Query e Key transposta
* NormalizaÃ§Ã£o por softmax
* AplicaÃ§Ã£o sobre os valores (Value)
* GeraÃ§Ã£o da nova representaÃ§Ã£o dos tokens

<h3 id="03.04">âš™ï¸ ImplementaÃ§Ã£o com pesos treinÃ¡veis</h3>
A versÃ£o mais completa da self-attention utiliza matrizes de pesos aprendÃ­veis para transformar os tokens de entrada em Q, K e V. Isso torna o mecanismo mais expressivo e ajustÃ¡vel.

<h3 id="03.04">ğŸ•¶ï¸ Causal Attention: ocultando palavras futuras</h3> 
Para tarefas de geraÃ§Ã£o de texto, Ã© importante que o modelo nÃ£o veja palavras do futuro. Para isso usamos causal attention, onde aplicamos uma mÃ¡scara triangular para bloquear os tokens Ã  frente:
* Apenas tokens anteriores (ou o prÃ³prio) sÃ£o considerados no cÃ¡lculo da atenÃ§Ã£o.
* Essencial para preservar o comportamento autoregressivo do modelo.

<h3 id="03.05">ğŸ”’ AplicaÃ§Ã£o da mÃ¡scara causal</h3> 
O uso da mÃ¡scara causal garante que:
* A saÃ­da no tempo t depende apenas de tokens atÃ© t.
* A previsÃ£o da prÃ³xima palavra nÃ£o vaza informaÃ§Ã£o futura.

<h3 id="03.05">ğŸŒ§ï¸ Dropout na atenÃ§Ã£o</h3> 
Durante o treinamento, Ã© comum aplicar dropout nos pesos de atenÃ§Ã£o, o que ajuda na regularizaÃ§Ã£o do modelo e evita overfitting.

<h3 id="03.06">ğŸ§© Multi-Head Attention: atenÃ§Ã£o paralela</h3> 
Em vez de usar uma Ãºnica funÃ§Ã£o de atenÃ§Ã£o, os modelos Transformers implementam multi-head attention:
* Dividem as representaÃ§Ãµes em mÃºltas "cabeÃ§as".
* Cada cabeÃ§a aprende padrÃµes diferentes da sequÃªncia.
* Os resultados sÃ£o concatenados e transformados em uma Ãºnica saÃ­da.

<h3 id="03.07">ğŸ§± Stacking de camadas de atenÃ§Ã£o</h3>
Empilhando vÃ¡rias camadas de multi-head attention, o modelo ganha profundidade e capacidade de abstraÃ§Ã£o, aprendendo:
* Sintaxe (em camadas inferiores)
* SemÃ¢ntica (em camadas superiores)

<h3 id="03.08">ğŸ“¦ ImplementaÃ§Ã£o Compacta</h3> 
As implementaÃ§Ãµes modernas encapsulam a lÃ³gica de atenÃ§Ã£o em classes compactas, como SelfAttention, CausalSelfAttention e MultiHeadAttention, permitindo reutilizaÃ§Ã£o e legibilidade do cÃ³digo.

<div style="text-align: right;">
  <a href="#">ğŸ” Voltar ao topo</a>
</div>

<h1 id="04">ğŸ”„ 4 - ExecuÃ§Ã£o do Modelo GPT-2 com TokenizaÃ§Ã£o e GeraÃ§Ã£o de Texto</h1>
Este mÃ³dulo demonstra como realizar a tokenizaÃ§Ã£o de frases, configurar um modelo GPT-2 com parÃ¢metros especÃ­ficos, e gerar novos textos a partir de um prompt inicial utilizando um modelo de linguagem (LLM) prÃ©-treinado.

<h3 id="04.01">ğŸ§  TokenizaÃ§Ã£o</h3> 
Textos de entrada sÃ£o transformados em tokens numÃ©ricos utilizando o tokenizer compatÃ­vel com o GPT-2. Esses tokens representam as palavras e subpalavras de forma que o modelo possa processÃ¡-los. SÃ£o adicionados tokens especiais, como inÃ­cio e fim de sequÃªncia, e os textos sÃ£o convertidos em tensores para posterior uso no modelo.

<h3 id="04.02">âš™ï¸ ConfiguraÃ§Ã£o do Modelo</h3> 
Ã‰ criada uma configuraÃ§Ã£o baseada no GPT-2 de 124 milhÃµes de parÃ¢metros, contendo:
* Tamanho do vocabulÃ¡rio
* Comprimento mÃ¡ximo de contexto (nÃºmero de tokens por entrada)
* DimensÃ£o dos embeddings
* NÃºmero de camadas de Transformer e cabeÃ§as de atenÃ§Ã£o
* Taxa de dropout, entre outros

Essa configuraÃ§Ã£o permite inicializar o modelo com as mesmas caracterÃ­sticas do GPT-2 original.

<h3 id="04.03">ğŸš€ ExecuÃ§Ã£o e SaÃ­da</h3> 
O modelo Ã© instanciado com os pesos definidos e executado sobre o batch de entrada, retornando uma matriz de logits â€” representaÃ§Ãµes de probabilidade de cada prÃ³ximo token possÃ­vel para cada posiÃ§Ã£o da sequÃªncia.

<h3 id="04.05">âœï¸ GeraÃ§Ã£o de Texto</h3> 
A partir de um prompt inicial, o modelo Ã© capaz de prever a prÃ³xima palavra/token com base no contexto anterior. Um loop iterativo permite a geraÃ§Ã£o de novos tokens atÃ© atingir um nÃºmero mÃ¡ximo ou um token de parada. O resultado final Ã© decodificado de volta para texto compreensÃ­vel.

Essa etapa mostra o pipeline completo de entrada, processamento e geraÃ§Ã£o de saÃ­da textual, simulando o comportamento bÃ¡sico de um modelo de linguagem autoregressivo como o GPT-2.

<div style="text-align: right;">
  <a href="#">ğŸ” Voltar ao topo</a>
</div>


<h1 id="05">âœ¨ 5 - AvaliaÃ§Ã£o e Treinamento de Modelos de Texto Generativo</h1>
Esta etapa do projeto explora como construir, avaliar, treinar e personalizar modelos de linguagem baseados na arquitetura GPT (Generative Pretrained Transformer). A proposta Ã© trabalhar desde a geraÃ§Ã£o de texto com modelos simples atÃ© o uso de modelos prÃ©-treinados, passando por estratÃ©gias de decodificaÃ§Ã£o e tÃ©cnicas de ajuste fino.

<h3 id="05.01">â­ AvaliaÃ§Ã£o de Modelos de Texto Generativo</h3> 
Nesta seÃ§Ã£o, Ã© demonstrado como gerar texto automaticamente a partir de um prompt inicial utilizando um modelo de linguagem. O processo consiste em fornecer uma frase inicial e deixar o modelo prever os prÃ³ximos tokens. A avaliaÃ§Ã£o do desempenho pode ser feita calculando a perda (loss) associada Ã s previsÃµes, alÃ©m de comparar os resultados gerados com os dados reais.

<h3 id="05.02">ğŸ’ª Treinamento de um LLM (Large Language Model)</h3> 
Esta parte mostra como treinar um modelo GPT a partir do zero. Para isso, Ã© feita a preparaÃ§Ã£o dos dados com divisÃ£o entre conjuntos de treinamento e validaÃ§Ã£o, configuraÃ§Ã£o dos hiperparÃ¢metros do modelo e uso de otimizadores adequados. SÃ£o utilizadas versÃµes ajustadas do GPT-2, com diferentes tamanhos e capacidades.

<h3 id="05.03">â™Ÿï¸ EstratÃ©gias de DecodificaÃ§Ã£o para Controlar Aleatoriedade</h3> 
Durante a geraÃ§Ã£o de texto, o modelo pode seguir diferentes estratÃ©gias para balancear criatividade e coerÃªncia. As principais tÃ©cnicas abordadas sÃ£o:
* Temperature Scaling: altera a distribuiÃ§Ã£o de probabilidade dos prÃ³ximos tokens, tornando a geraÃ§Ã£o mais ou menos imprevisÃ­vel.
* Top-k Sampling: limita a escolha dos prÃ³ximos tokens a um conjunto com os k mais provÃ¡veis, evitando escolhas muito aleatÃ³rias.

Essas tÃ©cnicas permitem personalizar a geraÃ§Ã£o conforme o objetivo, seja ele mais criativo ou mais conservador.

<h3 id="05.04">ğŸ—ï¸ Carregamento de Pesos PrÃ©-Treinados da OpenAI</h3> 
Por fim, Ã© demonstrado como carregar modelos jÃ¡ treinados pela OpenAI, como o GPT-2, diretamente no modelo implementado. Isso permite aproveitar redes neurais treinadas em grandes conjuntos de dados, sem necessidade de treinar do zero, economizando tempo e recursos computacionais.

<div style="text-align: right;">
  <a href="#">ğŸ” Voltar ao topo</a>
</div>


<h1 id="06">âœ‰ï¸ 06 - Spam</h1>
DetecÃ§Ã£o de Spam com LLM (GPT-2 Fine-Tuned)
Este projeto utiliza um modelo de linguagem grande (LLM) baseado no GPT-2 para a tarefa de detecÃ§Ã£o de spam. Combinando a capacidade contextual dos modelos de linguagem com uma camada de classificaÃ§Ã£o supervisionada, Ã© possÃ­vel transformar o GPT-2 em um poderoso classificador binÃ¡rio (spam ou nÃ£o spam).

* Preparando o conjunto de dados
* Criando um carregador de dados
* Inicializando um modelo com pesos prÃ©-treinado
* Adicionando uma classification head
* CÃ¡lculo da perda e precisÃ£o da classificaÃ§Ã£o
* Ajustando o modelo em dados supervisionados
* Usando o LLM como um classificador de spam

`Para testar com interface visual.`
```python
chainlit run 06.00-app.py
```

<div style="text-align: right;">
  <a href="#">ğŸ” Voltar ao topo</a>
</div>

<h1 id="07">ğŸ’ª 07 - IntruÃ§Ãµes para Treinamento</h1>
Este projeto realiza o fine-tuning de um modelo GPT-2 (ex: gpt2-small, gpt2-medium, etc.) para a tarefa de pergunta e resposta baseada em instruÃ§Ãµes (Instruction Tuning).

Ao contrÃ¡rio do treinamento tradicional que se baseia apenas na prediÃ§Ã£o da prÃ³xima palavra em sequÃªncias de texto, aqui utilizei um formato estruturado com instruÃ§Ãµes explÃ­citas. Esse formato guia o modelo a entender e seguir comandos, tornando-o mais eficaz para tarefas prÃ¡ticas como assistentes virtuais, automaÃ§Ã£o de suporte, geraÃ§Ã£o de conteÃºdo e muito mais.

* O processo de ajuste fino de instruÃ§Ãµes adapta um LLM prÃ©-treinado para seguir instruÃ§Ãµes humanas e gerar as respostas desejadas.
* A preparaÃ§Ã£o do conjunto de dados envolve o download de um conjunto de dados de instruÃ§Ãµes-respostas, a formataÃ§Ã£o das entradas e a divisÃ£o em conjuntos de treinamento, validaÃ§Ã£o e teste.
* Os lotes de treinamento sÃ£o construÃ­dos usando uma funÃ§Ã£o de agrupamento personalizada que preenche sequÃªncias, cria IDs de tokens de destino e mascara tokens de preenchimento.
* Carregamos um modelo GPT-2 prÃ©-treinado com 124 milhÃµes de parÃ¢metros para servir como ponto de partida para o ajuste fino das instruÃ§Ãµes.
* O modelo prÃ©-treinado Ã© ajustado no conjunto de dados de instruÃ§Ãµes usando um loop de treinamento semelhante ao prÃ©-treinamento.
* A avaliaÃ§Ã£o envolve a extraÃ§Ã£o das respostas do modelo em um conjunto de teste e sua pontuaÃ§Ã£o (por exemplo, usando outro LLM).


<div style="text-align: right;">
  <a href="#">ğŸ” Voltar ao topo</a>
</div>

<h1 id="08">ReferÃªncia</h1>
Build a Large Language Model - Sebastian Raschka<br>
Livros gratuitos para download: [Projeto Gutenberg](https://www.gutenberg.org/browse/languages/pt)<br>
Conjunto de dados utilizado para treinamento com pergunta e respostas: [Alpaca dataset - Stanford](https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json)


<div style="text-align: right;">
  <a href="#">ğŸ” Voltar ao topo</a>
</div>
