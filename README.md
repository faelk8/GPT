



# ğŸ”¤ 01 â€” TokenizaÃ§Ã£o
Antes que um modelo de linguagem possa processar texto, Ã© necessÃ¡rio transformar as palavras em tokens, ou seja, converter o texto em unidades menores que o modelo consegue entender e manipular. Isso Ã© feito atravÃ©s de um tokenizer. ApÃ³s o processamento, os tokens podem ser convertidos de volta para texto, permitindo reconstruir a saÃ­da gerada pelo modelo em linguagem natural.

### ğŸ”– Tokens especiais
Durante a tokenizaÃ§Ã£o, alguns tokens especiais sÃ£o usados para indicar posiÃ§Ãµes especÃ­ficas em uma sequÃªncia de texto:

* [BOS] (Beginning of Sequence) â€” Marca o inÃ­cio da sequÃªncia. Esse token informa ao modelo onde o conteÃºdo comeÃ§a.
* [EOS] (End of Sequence) â€” Indica o fim da sequÃªncia. Ele Ã© Ãºtil especialmente quando se juntam vÃ¡rios textos, como artigos ou sentenÃ§as diferentes. Semelhante ao token <|endoftext|> utilizado no GPT-2.
* [PAD] (Padding) â€” Usado para preencher textos mais curtos durante o treinamento em lotes. Quando sequÃªncias possuem tamanhos diferentes, os textos mais curtos sÃ£o estendidos com esse token para igualar o comprimento do lote.

### ğŸ§¬ TÃ©cnica de TokenizaÃ§Ã£o â€” Byte Pair Encoding (BPE)
O mÃ©todo mais comum utilizado na tokenizaÃ§Ã£o de modelos como o GPT-2 Ã© o Byte Pair Encoding (BPE). O BPE divide palavras em subpalavras ou caracteres com base em frequÃªncia de ocorrÃªncia. 
Palavras comuns sÃ£o representadas por menos tokens, enquanto palavras raras sÃ£o quebradas em partes menores.
Exemplo:
* A palavra desconectado pode ser tokenizada como: ["des", "con", "ect", "ado"].

Esse mÃ©todo Ã© eficiente porque reduz o vocabulÃ¡rio necessÃ¡rio e permite ao modelo lidar com palavras desconhecidas ou inventadas de maneira mais robusta.

# ğŸ§  02 - Attention
Camada de AtenÃ§Ã£o (Attention Layer)
O modelo GPT-2 Ã© baseado em uma arquitetura chamada Transformer, cujo principal componente Ã© a camada de atenÃ§Ã£o. Essa camada permite que o modelo "preste atenÃ§Ã£o" em diferentes partes da entrada enquanto estÃ¡ processando uma palavra ou token, capturando relaÃ§Ãµes de contexto de curto e longo alcance.

ğŸ§© Como funciona a atenÃ§Ã£o?
Durante o treinamento, cada token da entrada Ã© transformado em vetores chamados de query (Q), key (K) e value (V). A atenÃ§Ã£o Ã© calculada comparando as queries de um token com as keys dos outros tokens â€” permitindo que o modelo decida quais palavras sÃ£o mais relevantes para prever a prÃ³xima.

ğŸ¯ Multi-head Attention
Em vez de ter uma Ãºnica "cabeÃ§a de atenÃ§Ã£o", o Transformer pode usar vÃ¡rias cabeÃ§as (como 2, 4, 8 ou mais). Cada cabeÃ§a aprende padrÃµes diferentes de dependÃªncia entre palavras. Por exemplo:

* 1 cabeÃ§a: o modelo foca em um Ãºnico padrÃ£o de contexto.
* 2 cabeÃ§as: o modelo pode aprender dois padrÃµes distintos ao mesmo tempo, como relaÃ§Ãµes sintÃ¡ticas e semÃ¢nticas.

No GPT-2, essa atenÃ§Ã£o multi-cabeÃ§a Ã© uma das razÃµes pela qual ele entende tÃ£o bem o contexto de uma frase, mesmo quando as palavras estÃ£o distantes.

âš™ï¸ Exemplo prÃ¡tico
Se vocÃª estiver usando n_heads=2, o modelo divide o vetor de entrada em duas partes, aplica a atenÃ§Ã£o separadamente em cada uma, e depois concatena os resultados. Isso melhora a capacidade do modelo de capturar diferentes tipos de dependÃªncias linguÃ­sticas simultaneamente.

# ğŸ§  3 - O desafio de modelar sequÃªncias longas
Modelos de linguagem precisam lidar com sequÃªncias de texto de comprimento variÃ¡vel, mas muitos mÃ©todos tradicionais (como RNNs) sofrem com limitaÃ§Ãµes no alcance de dependÃªncias longas â€” ou seja, perdem informaÃ§Ãµes importantes quando os tokens estÃ£o muito distantes uns dos outros.<br>
O mecanismo de self-attention (captura dependÃªncias com mecanismos de atenÃ§Ã£o) resolve esse problema ao permitir que cada palavra "atenda" a todas as outras palavras da sequÃªncia, capturando relacionamentos globais entre os tokens de forma eficiente.

### ğŸ” Self-Attention: AtenÃ§Ã£o a diferentes partes da entrada
No self-attention, cada token gera uma consulta (query), chave (key) e valor (value), permitindo o cÃ¡lculo de pesos de atenÃ§Ã£o para todas as outras posiÃ§Ãµes da sequÃªncia. Com isso, o modelo consegue:
* Focar nos tokens relevantes
* Ignorar informaÃ§Ãµes irrelevantes
* Preservar contexto global em uma Ãºnica camada

### âœ… ImplementaÃ§Ã£o simples sem pesos treinÃ¡veis
VocÃª pode implementar uma versÃ£o bÃ¡sica de self-attention que apenas calcula as similaridades entre os tokens, sem ajustar pesos.

### ğŸ“Š CÃ¡lculo de pesos de atenÃ§Ã£o
O cÃ¡lculo dos pesos segue os seguintes passos:
* MultiplicaÃ§Ã£o de Query e Key transposta
* NormalizaÃ§Ã£o por softmax
* AplicaÃ§Ã£o sobre os valores (Value)
* GeraÃ§Ã£o da nova representaÃ§Ã£o dos tokens

## âš™ï¸ ImplementaÃ§Ã£o com pesos treinÃ¡veis
A versÃ£o mais completa da self-attention utiliza matrizes de pesos aprendÃ­veis para transformar os tokens de entrada em Q, K e V. Isso torna o mecanismo mais expressivo e ajustÃ¡vel.

### ğŸ•¶ï¸ Causal Attention: ocultando palavras futuras
Para tarefas de geraÃ§Ã£o de texto, Ã© importante que o modelo nÃ£o veja palavras do futuro. Para isso usamos causal attention, onde aplicamos uma mÃ¡scara triangular para bloquear os tokens Ã  frente:
* Apenas tokens anteriores (ou o prÃ³prio) sÃ£o considerados no cÃ¡lculo da atenÃ§Ã£o.
* Essencial para preservar o comportamento autoregressivo do modelo.

### ğŸ”’ AplicaÃ§Ã£o da mÃ¡scara causal
O uso da mÃ¡scara causal garante que:
* A saÃ­da no tempo t depende apenas de tokens atÃ© t.
* A previsÃ£o da prÃ³xima palavra nÃ£o vaza informaÃ§Ã£o futura.

### ğŸŒ§ï¸ Dropout na atenÃ§Ã£o
Durante o treinamento, Ã© comum aplicar dropout nos pesos de atenÃ§Ã£o, o que ajuda na regularizaÃ§Ã£o do modelo e evita overfitting.

### ğŸ§© Multi-Head Attention: atenÃ§Ã£o paralela
Em vez de usar uma Ãºnica funÃ§Ã£o de atenÃ§Ã£o, os modelos Transformers implementam multi-head attention:
* Dividem as representaÃ§Ãµes em mÃºltas "cabeÃ§as".
* Cada cabeÃ§a aprende padrÃµes diferentes da sequÃªncia.
* Os resultados sÃ£o concatenados e transformados em uma Ãºnica saÃ­da.

### ğŸ§± Stacking de camadas de atenÃ§Ã£o
Empilhando vÃ¡rias camadas de multi-head attention, o modelo ganha profundidade e capacidade de abstraÃ§Ã£o, aprendendo:
* Sintaxe (em camadas inferiores)
* SemÃ¢ntica (em camadas superiores)

### ğŸ“¦ ImplementaÃ§Ã£o Compacta
As implementaÃ§Ãµes modernas encapsulam a lÃ³gica de atenÃ§Ã£o em classes compactas, como SelfAttention, CausalSelfAttention e MultiHeadAttention, permitindo reutilizaÃ§Ã£o e legibilidade do cÃ³digo.


# ğŸ”„ 4 - ExecuÃ§Ã£o do Modelo GPT-2 com TokenizaÃ§Ã£o e GeraÃ§Ã£o de Texto
Este mÃ³dulo demonstra como realizar a tokenizaÃ§Ã£o de frases, configurar um modelo GPT-2 com parÃ¢metros especÃ­ficos, e gerar novos textos a partir de um prompt inicial utilizando um modelo de linguagem (LLM) prÃ©-treinado.

### ğŸ§  TokenizaÃ§Ã£o
Textos de entrada sÃ£o transformados em tokens numÃ©ricos utilizando o tokenizer compatÃ­vel com o GPT-2. Esses tokens representam as palavras e subpalavras de forma que o modelo possa processÃ¡-los. SÃ£o adicionados tokens especiais, como inÃ­cio e fim de sequÃªncia, e os textos sÃ£o convertidos em tensores para posterior uso no modelo.

### âš™ï¸ ConfiguraÃ§Ã£o do Modelo
Ã‰ criada uma configuraÃ§Ã£o baseada no GPT-2 de 124 milhÃµes de parÃ¢metros, contendo:
* Tamanho do vocabulÃ¡rio
* Comprimento mÃ¡ximo de contexto (nÃºmero de tokens por entrada)
* DimensÃ£o dos embeddings
* NÃºmero de camadas de Transformer e cabeÃ§as de atenÃ§Ã£o
* Taxa de dropout, entre outros

Essa configuraÃ§Ã£o permite inicializar o modelo com as mesmas caracterÃ­sticas do GPT-2 original.

### ğŸš€ ExecuÃ§Ã£o e SaÃ­da
O modelo Ã© instanciado com os pesos definidos e executado sobre o batch de entrada, retornando uma matriz de logits â€” representaÃ§Ãµes de probabilidade de cada prÃ³ximo token possÃ­vel para cada posiÃ§Ã£o da sequÃªncia.

### âœï¸ GeraÃ§Ã£o de Texto
A partir de um prompt inicial, o modelo Ã© capaz de prever a prÃ³xima palavra/token com base no contexto anterior. Um loop iterativo permite a geraÃ§Ã£o de novos tokens atÃ© atingir um nÃºmero mÃ¡ximo ou um token de parada. O resultado final Ã© decodificado de volta para texto compreensÃ­vel.

Essa etapa mostra o pipeline completo de entrada, processamento e geraÃ§Ã£o de saÃ­da textual, simulando o comportamento bÃ¡sico de um modelo de linguagem autoregressivo como o GPT-2.

# âœ‰ï¸ 06 - Spam
DetecÃ§Ã£o de Spam com LLM (GPT-2 Fine-Tuned)
Este projeto utiliza um modelo de linguagem grande (LLM) baseado no GPT-2 para a tarefa de detecÃ§Ã£o de spam. Combinando a capacidade contextual dos modelos de linguagem com uma camada de classificaÃ§Ã£o supervisionada, Ã© possÃ­vel transformar o GPT-2 em um poderoso classificador binÃ¡rio (spam ou nÃ£o spam).

* Preparando o conjunto de dados
* Criando um carregador de dados
* Inicializando um modelo com pesos prÃ©-treinado
* Adicionando uma classification head
* CÃ¡lculo da perda e precisÃ£o da classificaÃ§Ã£o
* Ajustando o modelo em dados supervisionados
* Usando o LLM como um classificador de spam

`Para testar com interface visual.`
```python
chainlit run 06.00-app.py
```

# ğŸ’ª 07 - IntruÃ§Ãµes para Treinamento
Este projeto realiza o fine-tuning de um modelo GPT-2 (ex: gpt2-small, gpt2-medium, etc.) para a tarefa de pergunta e resposta baseada em instruÃ§Ãµes (Instruction Tuning).

Ao contrÃ¡rio do treinamento tradicional que se baseia apenas na prediÃ§Ã£o da prÃ³xima palavra em sequÃªncias de texto, aqui utilizei um formato estruturado com instruÃ§Ãµes explÃ­citas. Esse formato guia o modelo a entender e seguir comandos, tornando-o mais eficaz para tarefas prÃ¡ticas como assistentes virtuais, automaÃ§Ã£o de suporte, geraÃ§Ã£o de conteÃºdo e muito mais.

* O processo de ajuste fino de instruÃ§Ãµes adapta um LLM prÃ©-treinado para seguir instruÃ§Ãµes humanas e gerar as respostas desejadas.
* A preparaÃ§Ã£o do conjunto de dados envolve o download de um conjunto de dados de instruÃ§Ãµes-respostas, a formataÃ§Ã£o das entradas e a divisÃ£o em conjuntos de treinamento, validaÃ§Ã£o e teste.
* Os lotes de treinamento sÃ£o construÃ­dos usando uma funÃ§Ã£o de agrupamento personalizada que preenche sequÃªncias, cria IDs de tokens de destino e mascara tokens de preenchimento.
* Carregamos um modelo GPT-2 prÃ©-treinado com 124 milhÃµes de parÃ¢metros para servir como ponto de partida para o ajuste fino das instruÃ§Ãµes.
* O modelo prÃ©-treinado Ã© ajustado no conjunto de dados de instruÃ§Ãµes usando um loop de treinamento semelhante ao prÃ©-treinamento.
* A avaliaÃ§Ã£o envolve a extraÃ§Ã£o das respostas do modelo em um conjunto de teste e sua pontuaÃ§Ã£o (por exemplo, usando outro LLM).




# ReferÃªncia
Build a Large Language Model - Sebastian Raschka<br>
Livros gratuitos para download: [Projeto Gutenberg](https://www.gutenberg.org/browse/languages/pt)<br>
Conjunto de dados utilizado para treinamento com pergunta e respostas: [Alpaca dataset - Stanford](https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json)

